{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers import Dense, Activation, Input\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_context(\"paper\")\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env=gym.make(\"CartPole-v1\")#LunarLander-v2 #CartPole-v1\n",
    "env._max_episode_steps=200\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_action():\n",
    "    return [1/n_actions]*n_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Iter=500\n",
    "G_log=[]\n",
    "for epoch in range(Iter):\n",
    "    done=False\n",
    "    s=env.reset()\n",
    "    G=0\n",
    "    while not done:\n",
    "        action_probas = random_action()\n",
    "        a = np.random.choice(n_actions, 1, p=action_probas)[0]\n",
    "        sp, r, done, info = env.step(a)\n",
    "        s = sp\n",
    "        G+=r\n",
    "    G_log.append(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v=plt.hist(G_log,bins=2*int(np.sqrt(len(G_log))))\n",
    "mean=np.mean(G_log)\n",
    "plt.vlines(mean,[0],[max(v[0])],label=\"mean={:.2f}\".format(mean))\n",
    "plt.legend()\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. REINFORCE (episodic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE(object):\n",
    "    def __init__(self,paras={}):\n",
    "        self.dim_state=paras['dim_state']\n",
    "        self.num_action=paras['num_action']\n",
    "        self.alpha_A=paras['alpha_A']\n",
    "        self.paras_A=np.zeros((self.dim_state,self.num_action))\n",
    "        self.gamma=0.999\n",
    "\n",
    "    def get_action_proba(self,s):\n",
    "        perfs=[self.h(s,b) for b in range(self.num_action)]\n",
    "        c=max(perfs)\n",
    "        tt=sum([np.exp(self.h(s,b)-c) for b in range(self.num_action)])\n",
    "        return [np.exp(self.h(s,a)-c)/tt for a in range(self.num_action)]\n",
    "    \n",
    "    def h(self,s,a):\n",
    "        return s.dot(self.paras_A)[a]\n",
    "    \n",
    "    def update_paras(self,states,actions,rewards):\n",
    "        T=len(states)\n",
    "        for t in range(T):\n",
    "            G=sum([rewards[k]*(self.gamma**(k-t-1)) for k in range(t+1,T)])\n",
    "            a=actions[t]\n",
    "            prob=self.get_action_proba(states[t])\n",
    "            for b in range(self.num_action):\n",
    "                self.paras_A[:,b]+=self.alpha_A*(self.gamma**t)*G*(int(a==b)-prob[b])*states[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent_config={}\n",
    "agent_config['dim_state']=state_dim[0]\n",
    "agent_config['num_action']=n_actions\n",
    "agent_config['alpha_A']=3e-2\n",
    "agent=REINFORCE(agent_config)\n",
    "\n",
    "Iter=500\n",
    "G_log=[]\n",
    "for epoch in range(Iter):\n",
    "    done=False\n",
    "    s=env.reset()\n",
    "    G=0\n",
    "    states=[]\n",
    "    actions=[]\n",
    "    rewards=[]\n",
    "    while not done:\n",
    "        action_probas = agent.get_action_proba(s)\n",
    "        a = np.random.choice(n_actions, 1, p=action_probas)[0]\n",
    "        sp, r, done, info = env.step(a)\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "        s = sp\n",
    "        G+=r\n",
    "    G_log.append(G)\n",
    "    agent.update_paras(states,actions,rewards)\n",
    "    \n",
    "    if (epoch+1)%100==0:\n",
    "        print(epoch+1,np.mean(G_log[-100:]))\n",
    "        \n",
    "plt.figure(figsize=(18,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(G_log)\n",
    "plt.plot(pd.DataFrame(G_log).rolling(100).mean())\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "v=plt.hist(G_log[-100:],bins=2*int(np.sqrt(len(G_log[-100:]))))\n",
    "mean=np.mean(G_log[-100:])\n",
    "plt.vlines(mean,[0],[max(v[0])],label=\"mean={:.2f}\".format(mean))\n",
    "plt.legend()\n",
    "plt.show()            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. REINFORCE with baseline (episodic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE_with_baseline(object):\n",
    "    def __init__(self,paras={}):\n",
    "        self.dim_state=paras['dim_state']\n",
    "        self.num_action=paras['num_action']\n",
    "        self.alpha_A=paras['alpha_A']\n",
    "        self.alpha_baseline=paras['alpha_baseline']\n",
    "        self.paras_A=np.zeros((self.dim_state,self.num_action))\n",
    "        self.paras_v=np.zeros(self.dim_state)\n",
    "        self.gamma=0.999\n",
    "    \n",
    "    def get_action_proba(self,s):\n",
    "        perfs=[self.h(s,b) for b in range(self.num_action)]\n",
    "        c=max(perfs)\n",
    "        tt=sum([np.exp(self.h(s,b)-c) for b in range(self.num_action)])\n",
    "        return [np.exp(self.h(s,a)-c)/tt for a in range(self.num_action)]\n",
    "    def v(self,s):\n",
    "        return s.dot(self.paras_v)\n",
    "    \n",
    "    def h(self,s,a):\n",
    "        return s.dot(self.paras_A)[a]\n",
    "    \n",
    "    def update_paras(self,states,actions,rewards):\n",
    "        T=len(states)\n",
    "        for t in range(T):\n",
    "            G=sum([rewards[k]*(self.gamma**(k-t-1)) for k in range(t+1,T)])\n",
    "            delta=G-self.v(states[t])\n",
    "            prob=self.get_action_proba(states[t])\n",
    "            self.paras_v+=self.alpha_baseline*delta*states[t]\n",
    "            for a in range(self.num_action):\n",
    "                self.paras_A[:,a]+=self.alpha_A*(self.gamma**t)*delta*(int(actions[t]==a)-prob[a])*states[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_config={}\n",
    "agent_config['dim_state']=state_dim[0]\n",
    "agent_config['num_action']=n_actions\n",
    "agent_config['alpha_A']=1e-2\n",
    "agent_config['alpha_baseline']=3e-2\n",
    "agent=REINFORCE_with_baseline(agent_config)\n",
    "Iter=500\n",
    "G_log=[]\n",
    "for epoch in range(Iter):\n",
    "    done=False\n",
    "    s=env.reset()\n",
    "    G=0\n",
    "    states=[]\n",
    "    actions=[]\n",
    "    rewards=[]\n",
    "    while not done:\n",
    "        action_probas = agent.get_action_proba(s)\n",
    "        a = np.random.choice(n_actions, 1, p=action_probas)[0]\n",
    "        sp, r, done, info = env.step(a)\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "        s = sp\n",
    "        G+=r\n",
    "    G_log.append(G)\n",
    "    agent.update_paras(states,actions,rewards)\n",
    "    \n",
    "    if (epoch+1)%100==0:\n",
    "        print(epoch+1,np.mean(G_log[-100:]))\n",
    "\n",
    "plt.figure(figsize=(18,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(G_log)\n",
    "plt.plot(pd.DataFrame(G_log).rolling(100).mean())\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "v=plt.hist(G_log[-100:],bins=2*int(np.sqrt(len(G_log[-100:]))))\n",
    "mean=np.mean(G_log[-100:])\n",
    "plt.vlines(mean,[0],[max(v[0])],label=\"mean={:.2f}\".format(mean))\n",
    "plt.legend()\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Actor-Critic (episodic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor_Critic(object):\n",
    "    def __init__(self,paras={}):\n",
    "        self.gamma=0.999\n",
    "        self.dim_state=paras['dim_state']\n",
    "        self.num_action=paras['num_action']\n",
    "        self.alpha_A=paras['alpha_A']\n",
    "        self.alpha_C=paras['alpha_C']\n",
    "        \n",
    "        state = Input(shape=[self.dim_state])\n",
    "        for idx,layer in enumerate(paras['structure']):\n",
    "            if idx==0:\n",
    "                dense_layers = [Dense(layer, activation='relu')(state)]\n",
    "            else:\n",
    "                dense_layers.append(Dense(layer, activation='relu')(dense_layers[-1]))\n",
    "\n",
    "        Qvalue = Dense(self.num_action, activation='linear')(dense_layers[-1])\n",
    "        value = Dense(1, activation='linear')(dense_layers[-1])\n",
    "        \n",
    "        critic = Model(input=[state], output=[value])\n",
    "        critic.compile(optimizer=Adam(lr=self.alpha_C), loss=\"mse\")\n",
    "        \n",
    "        actor = Model(input=[state], output=[Qvalue])\n",
    "        actor.compile(optimizer=Adam(lr=self.alpha_A), loss=\"mse\")\n",
    "        \n",
    "        self.actor=actor\n",
    "        self.critic=critic\n",
    "        self.Phi=K.function([self.actor.layers[0].input],[self.actor.layers[-2].output])         \n",
    "                \n",
    "    def v(self,s):\n",
    "        return self.critic.predict(s.reshape(1,-1))\n",
    "    \n",
    "    def get_action_proba(self,s):\n",
    "        prefs = self.actor.predict(s.reshape(1,-1))[0]\n",
    "        c=max(prefs)\n",
    "        tt=sum([np.exp(prefs[b]-c) for b in range(self.num_action)])\n",
    "        return [np.exp(prefs[a]-c)/tt for a in range(self.num_action)]\n",
    "    \n",
    "    def get_target_q_s_a(self,s,a,td_error,prob):\n",
    "        phi=self.Phi(s.reshape(1,-1))[0][0]\n",
    "        w,b=np.zeros(self.actor.get_weights()[-2].shape),np.zeros(self.actor.get_weights()[-1].shape)\n",
    "        for ap in range(self.num_action):\n",
    "            w[:,ap]+=(td_error*(int(a==ap)-prob[ap])*phi)[0]\n",
    "            b[ap]+=td_error*(int(a==ap)-prob[ap])\n",
    "        target_q_s_a=phi.dot(w)+b+self.actor.predict(s.reshape(1,-1))\n",
    "        return target_q_s_a\n",
    "    \n",
    "    def update_paras(self,s,a,sp,r,prob,done):\n",
    "        td_error=r+self.gamma*self.v(sp)*int(not done)-self.v(s)\n",
    "        critic_target=r+self.gamma*self.v(sp)*int(not done)\n",
    "        target_q_s_a=self.get_target_q_s_a(s,a,td_error,prob)\n",
    "        \n",
    "        self.actor.fit(s.reshape(1,-1), target_q_s_a.reshape(1,-1), verbose=0)\n",
    "        self.critic.fit(s.reshape(1,-1), critic_target, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor_Critic(object):\n",
    "    def __init__(self,paras):\n",
    "        self.gamma=0.999\n",
    "        self.dim_state=paras['dim_state']\n",
    "        self.num_action=paras['num_action']\n",
    "        self.alpha_A=paras['alpha_A']\n",
    "        self.actor_structure=paras['actor_structure']\n",
    "        self.alpha_C=paras['alpha_C']\n",
    "        self.critic_structure=paras['critic_structure']\n",
    "        self.actor,self.policy=self.build_actor()   \n",
    "        self.critic=self.build_critic()\n",
    "    \n",
    "    def build_actor(self):\n",
    "        td_error = Input(shape=[1,])\n",
    "        state = Input(shape=[self.dim_state,])\n",
    "        for idx,layer in enumerate(self.actor_structure):\n",
    "            if idx==0:\n",
    "                dense_layers = [Dense(layer, activation='relu')(state)]\n",
    "            else:\n",
    "                dense_layers.append(Dense(layer, activation='relu')(dense_layers[-1]))\n",
    "        pi = Dense(self.num_action, activation='softmax')(dense_layers[-1])\n",
    "        def custom_loss(y_true, prob):\n",
    "            loss = -K.log(prob)*y_true*td_error\n",
    "            return loss\n",
    "        actor = Model(input=[state,td_error], output=[pi])\n",
    "        actor.compile(optimizer=Adam(lr=self.alpha_A), loss=custom_loss)\n",
    "        policy = Model(input=[state], output=[pi])\n",
    "        return actor,policy\n",
    "    \n",
    "    def build_critic(self):\n",
    "        state = Input(shape=[self.dim_state,])\n",
    "        for idx,layer in enumerate(self.actor_structure):\n",
    "            if idx==0:\n",
    "                dense_layers = [Dense(layer, activation='relu')(state)]\n",
    "            else:\n",
    "                dense_layers.append(Dense(layer, activation='relu')(dense_layers[-1]))\n",
    "        value = Dense(1, activation='linear')(dense_layers[-1])\n",
    "        critic = Model(input=[state], output=[value])\n",
    "        critic.compile(optimizer=Adam(lr=self.alpha_C), loss=\"mse\")\n",
    "        return critic\n",
    "    \n",
    "    def v(self,s):\n",
    "        return self.critic.predict(s)\n",
    "    \n",
    "    def get_action_proba(self,s):\n",
    "        return self.policy.predict(s)[0]\n",
    "    \n",
    "    def learn(self,s,a,r,sp,done):  \n",
    "        td_error=r+self.gamma*self.v(sp.reshape(1,-1))*(1-done)-self.v(s.reshape(1,-1))\n",
    "        critic_target=r+self.gamma*self.v(sp.reshape(1,-1))*(1-done)\n",
    "        actions = np.zeros(self.num_action)\n",
    "        actions[a]=1\n",
    "        self.critic.fit(s.reshape(1,-1), critic_target.reshape(1,-1), verbose=0)\n",
    "        self.actor.fit([s.reshape(1,-1), td_error.reshape(1,-1)], actions.reshape(1,-1), verbose=0)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_config={}\n",
    "agent_config['dim_state']=state_dim[0]\n",
    "agent_config['num_action']=n_actions\n",
    "agent_config['alpha_A']=1e-5\n",
    "agent_config['alpha_C']=1e-3\n",
    "agent_config['actor_structure']=[128,128]\n",
    "agent_config['critic_structure']=[128,128]\n",
    "\n",
    "\n",
    "agent=Actor_Critic(agent_config)\n",
    "G_log=[]\n",
    "Iter=500\n",
    "for epoch in range(Iter):\n",
    "    done=False\n",
    "    s=env.reset()\n",
    "    G=0\n",
    "    while not done:\n",
    "        action_probas = agent.get_action_proba(s.reshape(1,-1))\n",
    "        a = np.random.choice(n_actions, 1, p=action_probas)[0]     \n",
    "        sp, r, done, info = env.step(a)\n",
    "        agent.learn(s,a,sp,r,done)\n",
    "        s = sp\n",
    "        G+=r\n",
    "    G_log.append(G)\n",
    "    if (epoch+1)%50==0:\n",
    "        print(epoch+1,np.mean(G_log[-100:]))\n",
    "\n",
    "plt.figure(figsize=(18,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(G_log)\n",
    "plt.plot(pd.DataFrame(G_log).rolling(100).mean())\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "v=plt.hist(G_log[-100:],bins=2*int(np.sqrt(len(G_log[-100:]))))\n",
    "mean=np.mean(G_log[-100:])\n",
    "plt.vlines(mean,[0],[max(v[0])],label=\"mean={:.2f}\".format(mean))\n",
    "plt.legend()\n",
    "plt.show()            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Actor-Critic with PER(episodic) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor_Critic_PER(object):\n",
    "    def __init__(self,paras):\n",
    "        self.gamma=0.999\n",
    "        self.memeory=[]\n",
    "        self.dim_state=paras['dim_state']\n",
    "        self.num_action=paras['num_action']\n",
    "        self.alpha_A=paras['alpha_A']\n",
    "        self.actor_structure=paras['actor_structure']\n",
    "        self.alpha_C=paras['alpha_C']\n",
    "        self.critic_structure=paras['critic_structure']\n",
    "        self.memeory_size=paras['memeory_size']\n",
    "        self.batch_size=paras['batch_size'] \n",
    "        self.alpha=paras['alpha']\n",
    "        self.actor,self.policy=self.build_actor()   \n",
    "        self.critic=self.build_critic()\n",
    "    \n",
    "    def build_actor(self):\n",
    "        td_error = Input(shape=[1,])\n",
    "        state = Input(shape=[self.dim_state,])\n",
    "        for idx,layer in enumerate(self.actor_structure):\n",
    "            if idx==0:\n",
    "                dense_layers = [Dense(layer, activation='relu')(state)]\n",
    "            else:\n",
    "                dense_layers.append(Dense(layer, activation='relu')(dense_layers[-1]))\n",
    "        pi = Dense(self.num_action, activation='softmax')(dense_layers[-1])\n",
    "        def custom_loss(prob_action, prob_taken_action):\n",
    "            prob_taken_action = K.clip(prob_taken_action, 1e-8, 1-1e-8)\n",
    "            log_lik = prob_action*K.log(prob_taken_action)\n",
    "            return K.sum(-log_lik*td_error)\n",
    "        actor = Model(input=[state,td_error], output=[pi])\n",
    "        actor.compile(optimizer=Adam(lr=self.alpha_A), loss=custom_loss)\n",
    "        policy = Model(input=[state], output=[pi])\n",
    "        return actor,policy\n",
    "    \n",
    "    def build_critic(self):\n",
    "        state = Input(shape=[self.dim_state,])\n",
    "        for idx,layer in enumerate(self.actor_structure):\n",
    "            if idx==0:\n",
    "                dense_layers = [Dense(layer, activation='relu')(state)]\n",
    "            else:\n",
    "                dense_layers.append(Dense(layer, activation='relu')(dense_layers[-1]))\n",
    "        value = Dense(1, activation='linear')(dense_layers[-1])\n",
    "        critic = Model(input=[state], output=[value])\n",
    "        critic.compile(optimizer=Adam(lr=self.alpha_C), loss=\"mse\")\n",
    "        return critic\n",
    "    \n",
    "    def v(self,s):\n",
    "        return self.critic.predict(s)\n",
    "    \n",
    "    def get_action_proba(self,s):\n",
    "        return self.policy.predict(s)[0]\n",
    "    \n",
    "    def update_paras(self,s,a,r,sp,done):  \n",
    "        td_error=r+self.gamma*self.v(sp)*(1-done)-self.v(s)\n",
    "        critic_target=r+self.gamma*self.v(sp)*(1-done)\n",
    "        actions = np.zeros([len(a), self.num_action])\n",
    "        for idx in range(len(a)):\n",
    "            actions[idx,a[idx][0]]=1\n",
    "        actions\n",
    "        self.critic.fit(s, critic_target, verbose=0)\n",
    "        self.actor.fit([s, td_error], actions, verbose=0)\n",
    "        \n",
    "    def memeorize(self,s,a,r,sp,done):\n",
    "        self.memeory.append([s,a,r,sp,done])\n",
    "        if len(self.memeory)>self.memeory_size:\n",
    "            del self.memeory[0]    \n",
    "    \n",
    "    def learn(self):\n",
    "        sampled_exp=self.sample_exp()\n",
    "        s=np.array([list(_) for _ in sampled_exp[:,0]])\n",
    "        a=sampled_exp[:,1].reshape(-1,1)\n",
    "        r=sampled_exp[:,2].reshape(-1,1)\n",
    "        sp=np.array([list(_) for _ in sampled_exp[:,3]])\n",
    "        done=sampled_exp[:,4].reshape(-1,1)\n",
    "        self.update_paras(s,a,r,sp,done)\n",
    "        \n",
    "    def sample_exp(self):\n",
    "        G=np.zeros(len(self.memeory))\n",
    "        end_idx=[-1]+list(np.where(np.array(self.memeory)[:,4]==True)[0])\n",
    "        R=np.array(self.memeory)[:,2]\n",
    "        if len(end_idx)>1:\n",
    "            for idx in range(1,len(end_idx)):\n",
    "                G[end_idx[idx-1]+1:end_idx[idx]+1]=sum(R[end_idx[idx-1]+1:end_idx[idx]+1])\n",
    "        G=(G-G.min()+1e-5)/(G.max()-G.min()+1e-5)        \n",
    "        prob=np.array(G)**self.alpha/sum(np.array(G)**self.alpha)\n",
    "        \n",
    "        index_set=np.random.choice(range(len(self.memeory)),\n",
    "                    size=min(len(self.memeory),self.batch_size),p=prob,replace=False)\n",
    "        return np.array(self.memeory)[index_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_config={}\n",
    "agent_config['dim_state']=state_dim[0]\n",
    "agent_config['num_action']=n_actions\n",
    "agent_config['alpha_A']=1e-5\n",
    "agent_config['alpha_C']=1e-3\n",
    "agent_config['memeory_size']=1e5\n",
    "agent_config['batch_size']=32\n",
    "agent_config['actor_structure']=[128,128]\n",
    "agent_config['critic_structure']=[128,128]\n",
    "agent_config['alpha']=0.9\n",
    "agent=Actor_Critic_PER(agent_config)\n",
    "\n",
    "Iter=200\n",
    "G_log=[]\n",
    "for epoch in range(Iter):\n",
    "    done=False\n",
    "    s=env.reset()\n",
    "    G=0\n",
    "    while not done:\n",
    "        action_probas = agent.get_action_proba(s.reshape(1,-1))\n",
    "        a = np.random.choice(n_actions, 1, p=action_probas)[0]     \n",
    "        sp, r, done, info = env.step(a)\n",
    "        agent.memeorize(s,a,r,sp,done)\n",
    "        agent.learn()\n",
    "        s = sp\n",
    "        G+=r\n",
    "    G_log.append(G)\n",
    "    if (epoch+1)%10==0:\n",
    "        print(epoch+1,np.mean(G_log[-100:]))\n",
    "        \n",
    "\n",
    "plt.plot(G_log)\n",
    "plt.plot(pd.DataFrame(G_log).rolling(100).mean())\n",
    "plt.show()            \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Average Reward Actor-Critic with PER(episodic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Average_Reward_Actor_Critic_PER(object):\n",
    "    def __init__(self,paras):\n",
    "        self.R=0\n",
    "        self.memeory=[]\n",
    "        self.dim_state=paras['dim_state']\n",
    "        self.num_action=paras['num_action']\n",
    "        self.alpha_A=paras['alpha_A']\n",
    "        self.actor_structure=paras['actor_structure']\n",
    "        self.alpha_C=paras['alpha_C']\n",
    "        self.critic_structure=paras['critic_structure']\n",
    "        self.alpha_R=paras['alpha_R']\n",
    "        self.memeory_size=paras['memeory_size']\n",
    "        self.batch_size=paras['batch_size'] \n",
    "        self.alpha=paras['alpha']\n",
    "        self.actor,self.policy=self.build_actor()   \n",
    "        self.critic=self.build_critic()\n",
    "    \n",
    "    def build_actor(self):\n",
    "        td_error = Input(shape=[1,])\n",
    "        state = Input(shape=[self.dim_state,])\n",
    "        for idx,layer in enumerate(self.actor_structure):\n",
    "            if idx==0:\n",
    "                dense_layers = [Dense(layer, activation='relu')(state)]\n",
    "            else:\n",
    "                dense_layers.append(Dense(layer, activation='relu')(dense_layers[-1]))\n",
    "        pi = Dense(self.num_action, activation='softmax')(dense_layers[-1])\n",
    "        def custom_loss(prob_action, prob_taken_action):\n",
    "            prob_taken_action = K.clip(prob_taken_action, 1e-8, 1-1e-8)\n",
    "            log_lik = prob_action*K.log(prob_taken_action)\n",
    "            return K.sum(-log_lik*td_error)\n",
    "        actor = Model(input=[state,td_error], output=[pi])\n",
    "        actor.compile(optimizer=Adam(lr=self.alpha_A), loss=custom_loss)\n",
    "        policy = Model(input=[state], output=[pi])\n",
    "        return actor,policy\n",
    "    \n",
    "    def build_critic(self):\n",
    "        state = Input(shape=[self.dim_state,])\n",
    "        for idx,layer in enumerate(self.actor_structure):\n",
    "            if idx==0:\n",
    "                dense_layers = [Dense(layer, activation='relu')(state)]\n",
    "            else:\n",
    "                dense_layers.append(Dense(layer, activation='relu')(dense_layers[-1]))\n",
    "        value = Dense(1, activation='linear')(dense_layers[-1])\n",
    "        critic = Model(input=[state], output=[value])\n",
    "        critic.compile(optimizer=Adam(lr=self.alpha_C), loss=\"mse\")\n",
    "        return critic\n",
    "    \n",
    "    def v(self,s):\n",
    "        return self.critic.predict(s)\n",
    "    \n",
    "    def get_action_proba(self,s):\n",
    "        return self.policy.predict(s)[0]\n",
    "    \n",
    "    def update_paras(self,s,a,r,sp,done): \n",
    "        td_error=r-self.R+self.v(sp)*(1-done)-self.v(s)\n",
    "        critic_target=r-self.R+self.v(sp)*(1-done)\n",
    "        self.R+=self.alpha_R*td_error.mean()\n",
    "        actions = np.zeros([len(a), self.num_action])\n",
    "        for idx in range(len(a)):\n",
    "            actions[idx,a[idx][0]]=1\n",
    "        self.critic.fit(s, critic_target, verbose=0)\n",
    "        self.actor.fit([s, td_error], actions, verbose=0)\n",
    "        \n",
    "    def memeorize(self,s,a,r,sp,done):\n",
    "        self.memeory.append([s,a,r,sp,done])\n",
    "        if len(self.memeory)>self.memeory_size:\n",
    "            del self.memeory[0]    \n",
    "    \n",
    "    def learn(self):\n",
    "        sampled_exp=self.sample_exp()\n",
    "        s=np.array([list(_) for _ in sampled_exp[:,0]])\n",
    "        a=sampled_exp[:,1].reshape(-1,1)\n",
    "        r=sampled_exp[:,2].reshape(-1,1)\n",
    "        sp=np.array([list(_) for _ in sampled_exp[:,3]])\n",
    "        done=sampled_exp[:,4].reshape(-1,1)\n",
    "        self.update_paras(s,a,r,sp,done)\n",
    "        \n",
    "    def sample_exp(self):\n",
    "        G=np.zeros(len(self.memeory))\n",
    "        end_idx=[-1]+list(np.where(np.array(self.memeory)[:,4]==True)[0])\n",
    "        R=np.array(self.memeory)[:,2]\n",
    "        if len(end_idx)>1:\n",
    "            for idx in range(1,len(end_idx)):\n",
    "                G[end_idx[idx-1]+1:end_idx[idx]+1]=sum(R[end_idx[idx-1]+1:end_idx[idx]+1])\n",
    "        G=(G-G.min()+1e-5)/(G.max()-G.min()+1e-5)        \n",
    "        prob=np.array(G)**self.alpha/sum(np.array(G)**self.alpha)\n",
    "        \n",
    "        index_set=np.random.choice(range(len(self.memeory)),\n",
    "                    size=min(len(self.memeory),self.batch_size),p=prob,replace=False)\n",
    "        return np.array(self.memeory)[index_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "agent_config={}\n",
    "agent_config['dim_state']=state_dim[0]\n",
    "agent_config['num_action']=n_actions\n",
    "agent_config['alpha_A']=1e-5\n",
    "agent_config['alpha_C']=1e-3\n",
    "agent_config['alpha_R']=1e-2\n",
    "agent_config['memeory_size']=1e5\n",
    "agent_config['batch_size']=32\n",
    "agent_config['actor_structure']=[128,128]\n",
    "agent_config['critic_structure']=[128,128]\n",
    "agent_config['alpha']=0.9\n",
    "agent=Average_Reward_Actor_Critic_PER(agent_config)\n",
    "\n",
    "Iter=200\n",
    "G_log=[]\n",
    "for epoch in range(Iter):\n",
    "    done=False\n",
    "    s=env.reset()\n",
    "    G=0\n",
    "    while not done:\n",
    "        action_probas = agent.get_action_proba(s.reshape(1,-1))\n",
    "        a = np.random.choice(n_actions, 1, p=action_probas)[0]     \n",
    "        sp, r, done, info = env.step(a)\n",
    "        agent.memeorize(s,a,r,sp,done)\n",
    "        agent.learn()\n",
    "        s = sp\n",
    "        G+=r\n",
    "    G_log.append(G)\n",
    "    if (epoch+1)%10==0:\n",
    "        print(epoch+1,np.mean(G_log[-100:]))\n",
    "        \n",
    "\n",
    "plt.plot(G_log)\n",
    "plt.plot(pd.DataFrame(G_log).rolling(100).mean())\n",
    "plt.show()            \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Average_Reward_Actor_Critic_PER(object):\n",
    "    def __init__(self,paras):\n",
    "        self.R=0\n",
    "        self.memeory=[]\n",
    "        self.dim_state=paras['dim_state']\n",
    "        self.num_action=paras['num_action']\n",
    "        self.alpha_A=paras['alpha_A']\n",
    "        self.actor_structure=paras['actor_structure']\n",
    "        self.alpha_C=paras['alpha_C']\n",
    "        self.critic_structure=paras['critic_structure']\n",
    "        self.alpha_R=paras['alpha_R']\n",
    "        self.memeory_size=paras['memeory_size']\n",
    "        self.batch_size=paras['batch_size'] \n",
    "        self.alpha=paras['alpha']\n",
    "        self.actor,self.Phi=self.build_actor()   \n",
    "        self.critic=self.build_critic()\n",
    "    \n",
    "    def build_actor(self):\n",
    "        td_error = Input(shape=[1,])\n",
    "        state = Input(shape=[self.dim_state,])\n",
    "        for idx,layer in enumerate(self.actor_structure):\n",
    "            if idx==0:\n",
    "                dense_layers = [Dense(layer, activation='relu')(state)]\n",
    "            else:\n",
    "                dense_layers.append(Dense(layer, activation='relu')(dense_layers[-1]))\n",
    "        Qvalue = Dense(self.num_action, activation='linear')(dense_layers[-1])\n",
    "        actor = Model(input=[state], output=[Qvalue])\n",
    "        actor.compile(optimizer=Adam(lr=self.alpha_A), loss=\"mse\")\n",
    "        Phi=K.function([actor.layers[0].input],[actor.layers[-2].output])  \n",
    "        return actor,Phi\n",
    "    \n",
    "    def build_critic(self):\n",
    "        state = Input(shape=[self.dim_state,])\n",
    "        for idx,layer in enumerate(self.actor_structure):\n",
    "            if idx==0:\n",
    "                dense_layers = [Dense(layer, activation='relu')(state)]\n",
    "            else:\n",
    "                dense_layers.append(Dense(layer, activation='relu')(dense_layers[-1]))\n",
    "        value = Dense(1, activation='linear')(dense_layers[-1])\n",
    "        critic = Model(input=[state], output=[value])\n",
    "        critic.compile(optimizer=Adam(lr=self.alpha_C), loss=\"mse\")\n",
    "        return critic\n",
    "    \n",
    "    def v(self,s):\n",
    "        return self.critic.predict(s)\n",
    "    \n",
    "    def get_action_proba(self,s):\n",
    "        prefs = self.actor.predict(s)\n",
    "        c=prefs.max(axis=1)\n",
    "        prob=np.exp(prefs-c.reshape(-1,1))/np.exp(prefs-c.reshape(-1,1)).sum(axis=1).reshape(-1,1)\n",
    "        return prob\n",
    "    \n",
    "    def get_target_q_s_a(self,s,a,td_error):\n",
    "        prob=self.get_action_proba(s)\n",
    "        phi=self.Phi(s)[0]\n",
    "        w=np.zeros(list(self.actor.get_weights()[-2].shape)+[s.shape[0]])\n",
    "        b=np.zeros(list(self.actor.get_weights()[-1].shape)+[s.shape[0]])\n",
    "        \n",
    "        for ap in range(self.num_action):\n",
    "            temp=(np.array([1 if _==ap else 0 for _ in a])-prob[:,ap]).reshape(-1,1)\n",
    "            w[:,ap,:]=w[:,ap,:]+(td_error*temp*phi).T\n",
    "            b[ap,:]=b[ap,:]+(td_error*temp).reshape(1,-1)\n",
    "        target_q_s_a=np.array([list(phi[idx,:].dot(w[:,:,idx])) for idx in range(len(s))])+b.T+self.actor.predict(s)\n",
    "        return target_q_s_a\n",
    "    def update_paras(self,s,a,r,sp,done): \n",
    "        td_error=r-self.R+self.v(sp)*(1-done)-self.v(s)\n",
    "        critic_target=r-self.R+self.v(sp)*(1-done)\n",
    "        self.R+=self.alpha_R*td_error.mean()\n",
    "        \n",
    "        self.critic.fit(s, critic_target, verbose=0)\n",
    "                \n",
    "        target_q_s_a=self.get_target_q_s_a(s,a,td_error)\n",
    "        self.actor.fit(s, target_q_s_a, verbose=0)\n",
    "        \n",
    "    def memeorize(self,s,a,r,sp,done):\n",
    "        self.memeory.append([s,a,r,sp,done])\n",
    "        if len(self.memeory)>self.memeory_size:\n",
    "            del self.memeory[0]    \n",
    "    \n",
    "    def learn(self):\n",
    "        sampled_exp=self.sample_exp()\n",
    "        s=np.array([list(_) for _ in sampled_exp[:,0]])\n",
    "        a=sampled_exp[:,1].reshape(-1,1)\n",
    "        r=sampled_exp[:,2].reshape(-1,1)\n",
    "        sp=np.array([list(_) for _ in sampled_exp[:,3]])\n",
    "        done=sampled_exp[:,4].reshape(-1,1)\n",
    "        self.update_paras(s,a,r,sp,done)\n",
    "        \n",
    "    def sample_exp(self):\n",
    "        G=np.zeros(len(self.memeory))\n",
    "        end_idx=[-1]+list(np.where(np.array(self.memeory)[:,4]==True)[0])\n",
    "        R=np.array(self.memeory)[:,2]\n",
    "        if len(end_idx)>1:\n",
    "            for idx in range(1,len(end_idx)):\n",
    "                G[end_idx[idx-1]+1:end_idx[idx]+1]=sum(R[end_idx[idx-1]+1:end_idx[idx]+1])\n",
    "        G=(G-G.min()+1e-5)/(G.max()-G.min()+1e-5)        \n",
    "        prob=np.array(G)**self.alpha/sum(np.array(G)**self.alpha)\n",
    "        \n",
    "        index_set=np.random.choice(range(len(self.memeory)),\n",
    "                    size=min(len(self.memeory),self.batch_size),p=prob,replace=False)\n",
    "        return np.array(self.memeory)[index_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_config={}\n",
    "agent_config['dim_state']=state_dim[0]\n",
    "agent_config['num_action']=n_actions\n",
    "agent_config['alpha_A']=1e-5\n",
    "agent_config['alpha_C']=1e-3\n",
    "agent_config['alpha_R']=1e-2\n",
    "agent_config['memeory_size']=1e5\n",
    "agent_config['batch_size']=32\n",
    "agent_config['actor_structure']=[128,128]\n",
    "agent_config['critic_structure']=[128,128]\n",
    "agent_config['alpha']=0.9\n",
    "agent=Average_Reward_Actor_Critic_PER(agent_config)\n",
    "\n",
    "\n",
    "Iter=200\n",
    "G_log=[]\n",
    "for epoch in range(Iter):\n",
    "    done=False\n",
    "    s=env.reset()\n",
    "    G=0\n",
    "    while not done:\n",
    "        action_probas = agent.get_action_proba(s.reshape(1,-1))[0]\n",
    "        a = np.random.choice(n_actions, 1, p=action_probas)[0]     \n",
    "        sp, r, done, info = env.step(a)\n",
    "        agent.memeorize(s,a,r,sp,done)\n",
    "        agent.learn()\n",
    "        s = sp\n",
    "        G+=r\n",
    "    G_log.append(G)\n",
    "    if (epoch+1)%10==0:\n",
    "        print(epoch+1,np.mean(G_log[-100:]))\n",
    "        \n",
    "\n",
    "plt.plot(G_log)\n",
    "plt.plot(pd.DataFrame(G_log).rolling(100).mean())\n",
    "plt.show()            \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_config={}\n",
    "agent_config['dim_state']=state_dim[0]\n",
    "agent_config['num_action']=n_actions\n",
    "agent_config['alpha_A']=1e-5\n",
    "agent_config['alpha_C']=1e-3\n",
    "agent_config['alpha_R']=1e-2\n",
    "agent_config['memeory_size']=1e5\n",
    "agent_config['batch_size']=32\n",
    "agent_config['actor_structure']=[128,128]\n",
    "agent_config['critic_structure']=[128,128]\n",
    "agent_config['alpha']=0.75\n",
    "agent=Average_Reward_Actor_Critic_PER(agent_config)\n",
    "\n",
    "\n",
    "Iter=200\n",
    "G_log=[]\n",
    "for epoch in range(Iter):\n",
    "    done=False\n",
    "    s=env.reset()\n",
    "    G=0\n",
    "    while not done:\n",
    "        action_probas = agent.get_action_proba(s.reshape(1,-1))[0]\n",
    "        a = np.random.choice(n_actions, 1, p=action_probas)[0]     \n",
    "        sp, r, done, info = env.step(a)\n",
    "        agent.memeorize(s,a,r,sp,done)\n",
    "        agent.learn()\n",
    "        s = sp\n",
    "        G+=r\n",
    "    G_log.append(G)\n",
    "    if (epoch+1)%10==0:\n",
    "        print(epoch+1,np.mean(G_log[-100:]))\n",
    "        \n",
    "\n",
    "plt.plot(G_log)\n",
    "plt.plot(pd.DataFrame(G_log).rolling(100).mean())\n",
    "plt.show()            \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
